{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 43043: expected 2 fields, saw 3\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews:  74998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 10\n",
    "traindf=[]\n",
    "traindf = pd.read_csv('labeledTrainData.tsv', delimiter=\"\\t\")\n",
    "traindf = traindf.drop(['id'], axis=1)\n",
    "traindf['type'] = 'train'\n",
    "addtldf=[]\n",
    "addtldf = pd.read_csv('unlabeledTrainData.tsv',error_bad_lines=False,delimiter=\"\\t\")\n",
    "addtldf = addtldf.drop(['id'], axis=1)\n",
    "addtldf['type'] = 'addtl'\n",
    "revcorpus = pd.concat([traindf,addtldf],ignore_index=True)\n",
    "print(\"Number of reviews: \",len(revcorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews:  74998\n",
      "      review  sentiment   type processed_review\n",
      "0  With a...        1.0  train  stuff ...      \n",
      "1  \\The C...        1.0  train  classi...      \n",
      "2  The fi...        0.0  train  film s...      \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "# import the inflect library \n",
    "import inflect \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams,bigrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "p = inflect.engine() \n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "# extract pure text from html\n",
    "def remove_tags(text):\n",
    "    text = re.sub(r'<[^<>]+>', \" \", text)\n",
    "    return text\n",
    "# remove stopwords function \n",
    "def remove_stopwords(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    filtered_text = \" \".join(filtered_text)\n",
    "    return filtered_text \n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "  \n",
    "# convert number into words \n",
    "def convert_number(text): \n",
    "    # split string into list of words \n",
    "    temp_str = text.split() \n",
    "    # initialise empty list \n",
    "    new_string = [] \n",
    "  \n",
    "    for word in temp_str: \n",
    "        # if word is a digit, convert the digit \n",
    "        # to numbers and append into the new_string list \n",
    "        if word.isdigit(): \n",
    "            temp = p.number_to_words(word) \n",
    "            new_string.append(temp) \n",
    "  \n",
    "        # append the word as it is \n",
    "        else: \n",
    "            new_string.append(word) \n",
    "  \n",
    "    # join the words of new_string to form a string \n",
    "    temp_str = ' '.join(new_string) \n",
    "    return temp_str\n",
    "\n",
    "# remove punctuation \n",
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator)\n",
    "\n",
    "    \n",
    "# remove whitespace from text \n",
    "def remove_whitespace(text): \n",
    "    return  \" \".join(text.split()) \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "# lemmatize string \n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    # provide context i.e. part-of-speech \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    lemmas = \" \".join(lemmas)\n",
    "    return lemmas \n",
    "# stem words in the list of tokenised words \n",
    "def stem_words(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stemmer.stem(word) for word in word_tokens] \n",
    "    stems = \" \".join(stems)\n",
    "    return stems \n",
    "\n",
    "def generate_ngram(text,n):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    tokens = word_tokenize(text) \n",
    "    output = list(ngrams(tokens, n))\n",
    "    #output = bigrams(tokens)\n",
    "    return output\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = remove_tags(text)\n",
    "    text = convert_number(text)\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_whitespace(text)\n",
    "    #text = stem_words(text)\n",
    "    #text = ngram_vector(text)\n",
    "    #print(text)\n",
    "    text = lemmatize_word(text)\n",
    "    return text\n",
    "#revcorpus = revcorpus[1:5]\n",
    "revcorpus['processed_review'] = revcorpus.review.apply(lambda x: preprocess_text(x))  \n",
    "#revcorpus['bigrams'] = revcorpus.processed_review.apply(lambda x:generate_ngram(x,2))\n",
    "print(\"Number of reviews: \",len(revcorpus))\n",
    "print(revcorpus.head(3))\n",
    "#traindf['processed_review'] = traindf.review.apply(lambda x: preprocess_text(x))  \n",
    "#print(\"Number of reviews: \",len(traindf))\n",
    "#traindf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'start', 'manager', 'nicholas', 'bell', 'give', 'welcome', 'investors', 'robert_carradine', 'primal', 'park', 'secret', 'project', 'mutate', 'primal', 'animal', 'use', 'fossilize', 'dna', 'like', '¨jurassik', 'park¨', 'scientists', 'resurrect', 'one', 'natures', 'fearsome', 'predators', 'sabretooth', 'tiger', 'smilodon', 'scientific', 'ambition', 'turn', 'deadly', 'however', 'high_voltage', 'fence', 'open', 'creature', 'escape', 'begin', 'savagely', 'stalk_prey', 'human', 'visitors', 'tourists', 'scientificmeanwhile', 'youngsters', 'enter', 'restrict_area', 'security', 'center', 'attack', 'pack', 'large', 'prehistorical', 'animals', 'deadlier', 'bigger', 'addition', 'security', 'agent', 'stacy', 'haiduk', 'mate', 'brian', 'wimmer', 'fight', 'hardly', 'carnivorous', 'smilodons', 'sabretooths', 'course', 'real', 'star', 'star', 'astound', 'terrifyingly', 'though', 'convince', 'giant', 'animals', 'savagely', 'stalk_prey', 'group', 'run_afoul', 'fight', 'one', 'natures', 'fearsome', 'predators', 'furthermore', 'third', 'sabretooth', 'dangerous', 'slow', 'stalk', 'victims', 'movie', 'deliver_goods', 'lot', 'blood_gore', 'behead', 'hairraising', 'chillsfull', 'scar', 'sabretooths', 'appear', 'mediocre', 'special', 'effectsthe', 'story', 'provide', 'excite', 'stir', 'entertainment', 'result', 'quite', 'bore', 'giant', 'animals', 'majority', 'make', 'computer', 'generator', 'seem', 'totally', 'lousy', 'middle', 'performances', 'though', 'players', 'react', 'appropriately', 'become', 'foodactors', 'give', 'vigorously', 'physical', 'performances', 'dodge', 'beasts', 'runningbound', 'leap', 'dangle', 'wall', 'pack', 'ridiculous', 'final', 'deadly', 'scene', 'small', 'kid', 'realisticgory', 'violent', 'attack', 'scenes', 'film', 'sabretooths', 'smilodon', 'follow', '¨sabretooth2002¨by', 'jam', 'r', 'hickox', 'vanessa_angel', 'david_keith', 'john_rhys', 'davies', 'much_better', '¨10000', 'bc2006¨', 'roland_emmerich', 'steven', 'strait', 'cliff', 'curtis', 'camilla', 'belle', 'motion_picture', 'fill', 'bloody', 'moments', 'badly_direct', 'george', 'miller', 'originality', 'take', 'many', 'elements', 'previous', 'film', 'miller', 'australian', 'director', 'usually', 'work', 'television', 'tidal_wave', 'journey_center', 'earth', 'many_others', 'occasionally', 'cinema', 'man_snowy', 'river', 'zeus', 'roxannerobinson', 'crusoe', 'rat', 'average', 'bottom_barrel']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "reviews = revcorpus.processed_review.apply(lambda x: word_tokenize(x))  \n",
    "bigrams = Phrases(sentences=reviews)\n",
    "print(bigrams[reviews][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = bigrams[reviews]\n",
    "reviews = [\" \".join(text) for text in [review for review in reviews]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classic war_worlds timothy_hines entertain film obviously go great effort lengths faithfully recreate h_g well classic book mr_hines succeed watch film appreciate fact standard predictable hollywood_fare come every year eg spielberg version tom_cruise slightest resemblance book obviously everyone look different things movie envision amateur critics look criticize everything others rate movie important baseslike entertain people never agree critics enjoy effort mr_hines put faithful hg_well classic_novel find entertain make easy_overlook critics perceive shortcomings'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#BAG OF WORDS MODEL\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "max_features = 3000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(reviews)\n",
    "maxlen = 200\n",
    "X_pad = pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,  423,   11,  478,  121,   82,  972,\n",
       "        145,   12,  931,  551,   12,   12,  193,   46,    7,  650, 2230,\n",
       "         70,   15,   16,  508,  193,    4,  227,  662, 2134, 1168,   58,\n",
       "         58,  271,    1,  245,   11,    6,  372, 1588,  276, 1174,  559,\n",
       "         59,  775, 2026,   27,  483,  559,  715,   26, 1904, 1032,  187,\n",
       "        456, 1734,  816, 2138,    5,  485,   11,  654,   38,  176,  119,\n",
       "        169,    4,    2,  114,   14,   23,    4,  114,  206,   16,  258,\n",
       "        727,  271,    1,  115,  347,   82,  652,  147, 1496,  317,  808,\n",
       "        529,  801,  715, 1356,   46,  263,   26,  583,  517,  808,   10,\n",
       "         46,   25,   24, 2671,  715,  452,  193,  743,  145,   64,  508,\n",
       "         97,    5,   92,  437, 1990,  135, 1602, 2450,  317,   27,   91,\n",
       "        134,   37,    1,   26,  317,  523,  598,  654,   44,    3,  141,\n",
       "        163,  524,  135,  628,  929, 1112,  458,   56, 1200,  118,    2,\n",
       "         25,    5,    3,  438,   75,   15,   25,  416,  160,   48,   31,\n",
       "        559, 2098,    2,  167,  456, 1734,  292,    3,  914,   25,   52,\n",
       "       1281, 1189, 2134,   19,  568,  121,   31,   19,   35,   24,   25,\n",
       "        185,  433,  364,   24,  105,  275,  467,  258,  299,   70,    3,\n",
       "        335, 1491])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pad = X_pad[0:25000]\n",
    "y = revcorpus[revcorpus['type']==\"train\"]['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1005 11:41:00.244666  9960 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1005 11:41:00.262618  9960 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1005 11:41:00.264613  9960 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1005 11:41:01.101415  9960 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1005 11:41:01.112384  9960 deprecation.py:506] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 200)         600000    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, None, 64)          68096     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 669,417\n",
      "Trainable params: 669,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten,CuDNNLSTM, CuDNNGRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "embed_size = 200\n",
    "model = Sequential()\n",
    "#model.add(Embedding(input_dim=wordvec_model.wv.vectors.shape[0],output_dim=wordvec_model.wv.vectors.shape[1],weights=[wordvec_model.wv.vectors], input_length=embed_size))\n",
    "model.add(Embedding(max_features,embed_size))\n",
    "model.add(CuDNNLSTM(64,return_sequences = True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   11,   63,   76],\n",
       "       [   0,    0,    0, ...,  549,  126,  203],\n",
       "       [   0,    0,    0, ...,   19,  290, 2329],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    2,  209,    8],\n",
       "       [   0,    0,    0, ...,  905,  147,   63],\n",
       "       [   0,    0,    0, ...,    1,  316,   84]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, shuffle=True)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 14s 685us/step - loss: 0.2268 - acc: 0.9110\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 13s 661us/step - loss: 0.1370 - acc: 0.9519\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 13s 661us/step - loss: 0.1050 - acc: 0.9673\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 13s 660us/step - loss: 0.0836 - acc: 0.9750\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 13s 663us/step - loss: 0.0674 - acc: 0.9813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20f1d0fb588>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "model.compile(loss='binary_crossentropy', optimizer='AdaGrad', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 2s 301us/step\n",
      "\n",
      "Test accuracy: 85.9%\n"
     ]
    }
   ],
   "source": [
    "# validate the model on test dataset to determine generalization\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
