{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 10\n",
    "traindf=[]\n",
    "traindf = pd.read_csv('labeledTrainData.tsv', delimiter=\"\\t\")\n",
    "traindf = traindf.drop(['id'], axis=1)\n",
    "traindf['type'] = 'train'\n",
    "addtldf=[]\n",
    "addtldf = pd.read_csv('unlabeledTrainData.tsv',error_bad_lines=False,delimiter=\"\\t\")\n",
    "addtldf = addtldf.drop(['id'], axis=1)\n",
    "addtldf['type'] = 'addtl'\n",
    "revcorpus = pd.concat([traindf,addtldf],ignore_index=True)\n",
    "print(\"Number of reviews: \",len(revcorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "# import the inflect library \n",
    "import inflect \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams,bigrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "p = inflect.engine() \n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "# extract pure text from html\n",
    "def remove_tags(text):\n",
    "    text = re.sub(r'<[^<>]+>', \" \", text)\n",
    "    return text\n",
    "# remove stopwords function \n",
    "def remove_stopwords(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    filtered_text = \" \".join(filtered_text)\n",
    "    return filtered_text \n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "  \n",
    "# convert number into words \n",
    "def convert_number(text): \n",
    "    # split string into list of words \n",
    "    temp_str = text.split() \n",
    "    # initialise empty list \n",
    "    new_string = [] \n",
    "  \n",
    "    for word in temp_str: \n",
    "        # if word is a digit, convert the digit \n",
    "        # to numbers and append into the new_string list \n",
    "        if word.isdigit(): \n",
    "            temp = p.number_to_words(word) \n",
    "            new_string.append(temp) \n",
    "  \n",
    "        # append the word as it is \n",
    "        else: \n",
    "            new_string.append(word) \n",
    "  \n",
    "    # join the words of new_string to form a string \n",
    "    temp_str = ' '.join(new_string) \n",
    "    return temp_str\n",
    "\n",
    "# remove punctuation \n",
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator)\n",
    "\n",
    "    \n",
    "# remove whitespace from text \n",
    "def remove_whitespace(text): \n",
    "    return  \" \".join(text.split()) \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "# lemmatize string \n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    # provide context i.e. part-of-speech \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    #lemmas = \" \".join(lemmas)\n",
    "    return lemmas \n",
    "# stem words in the list of tokenised words \n",
    "def stem_words(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stemmer.stem(word) for word in word_tokens] \n",
    "    stems = \" \".join(stems)\n",
    "    return stems \n",
    "\n",
    "def generate_ngram(text,n):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    tokens = word_tokenize(text) \n",
    "    output = list(ngrams(tokens, n))\n",
    "    #output = bigrams(tokens)\n",
    "    return output\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = remove_tags(text)\n",
    "    text = convert_number(text)\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_whitespace(text)\n",
    "    #text = stem_words(text)\n",
    "    #text = ngram_vector(text)\n",
    "    #print(text)\n",
    "    text = lemmatize_word(text)\n",
    "    return text\n",
    "#revcorpus = revcorpus[1:5]\n",
    "revcorpus['processed_review'] = revcorpus.review.apply(lambda x: preprocess_text(x))  \n",
    "#revcorpus['bigrams'] = revcorpus.processed_review.apply(lambda x:generate_ngram(x,2))\n",
    "print(\"Number of reviews: \",len(revcorpus))\n",
    "print(revcorpus.head(3))\n",
    "#traindf['processed_review'] = traindf.review.apply(lambda x: preprocess_text(x))  \n",
    "#print(\"Number of reviews: \",len(traindf))\n",
    "#traindf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "reviews = revcorpus['processed_review']\n",
    "bigrams = Phrases(sentences=reviews)\n",
    "print(bigrams[revcorpus['processed_review']][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAG OF WORDS MODEL\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "max_features = 3000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(traindf['review'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(traindf['review'])\n",
    "maxlen = 200\n",
    "X_pad = pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten,CuDNNLSTM, CuDNNGRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "embed_size = 150\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=wordvec_model.wv.vectors.shape[0],output_dim=wordvec_model.wv.vectors.shape[1],weights=[wordvec_model.wv.vectors], input_length=embed_size))\n",
    "#model.add(Embedding(max_features,embed_size))\n",
    "model.add(Bidirectional(CuDNNGRU(32,return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, shuffle=True)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "model.compile(loss='binary_crossentropy', optimizer='AdaGrad', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the model on test dataset to determine generalization\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
