{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#import review corpus\n",
    "y = pd.read_csv('revcorpus.csv')\n",
    "\n",
    "#import word2vec model\n",
    "embedding_vector_size = 300\n",
    "wordvec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "#import xpad\n",
    "X_pad = np.load('xpad.npy',allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten,CuDNNLSTM, CuDNNGRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "embed_size = 150\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=wordvec_model.wv.vectors.shape[0],\n",
    "                    output_dim=wordvec_model.wv.vectors.shape[1],\n",
    "                    weights=[wordvec_model.wv.vectors],\n",
    "                    input_length=embed_size))\n",
    "\n",
    "#model.add(Embedding(max_features,embed_size))\n",
    "\n",
    "model.add(Bidirectional(CuDNNLSTM(64,return_sequences = True)))\n",
    "\n",
    "model.add(GlobalMaxPool1D())\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "    \n",
    "model.add(Dense(128,\n",
    "                #activation=keras.layers.LeakyReLU(alpha=0.2),\n",
    "                use_bias=True,\n",
    "                kernel_initializer='lecun_uniform',\n",
    "                bias_initializer='lecun_uniform',\n",
    "                kernel_regularizer=keras.regularizers.l2(0.00),\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                #kernel_constraint=keras.constraints.MaxNorm(2.),\n",
    "                bias_constraint=None))\n",
    "\n",
    "model.add(Activation(keras.layers.LeakyReLU(alpha=0.3)))\n",
    "\n",
    "model.add(Dropout(0.00))\n",
    "\n",
    "model.add(Dense(32, \n",
    "                #activation=keras.layers.LeakyReLU(alpha=0.2),\n",
    "                use_bias=True,\n",
    "                kernel_initializer='lecun_uniform',\n",
    "                bias_initializer='lecun_uniform',\n",
    "                kernel_regularizer=keras.regularizers.l2(0.00),\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                #kernel_constraint=keras.constraints.MaxNorm(2.),\n",
    "                bias_constraint=None))\n",
    "\n",
    "model.add(Activation(keras.layers.LeakyReLU(alpha=0.2)))\n",
    "\n",
    "model.add(Dropout(0.05))\n",
    "\n",
    "model.add(Dense(1,\n",
    "                activation='sigmoid',\n",
    "                use_bias=True,\n",
    "                kernel_initializer='lecun_uniform',\n",
    "                bias_initializer='lecun_uniform',\n",
    "                kernel_regularizer=keras.regularizers.l2(0.00),\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                #kernel_constraint=keras.constraints.MaxNorm(2.),\n",
    "                bias_constraint=None))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, shuffle=True)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "# validate the model on test dataset to determine generalization\n",
    "print(\"\\n\")\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensoFlow",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
