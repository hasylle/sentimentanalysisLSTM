{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from pandas) (2019.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from sklearn) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from scikit-learn->sklearn) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from scikit-learn->sklearn) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from gensim) (1.8.4)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from gensim) (1.16.4)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from gensim) (1.3.1)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.9.238)\n",
      "Requirement already satisfied: requests in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.238 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.12.238)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.6.16)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from botocore<1.13.0,>=1.12.238->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\ellysah\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from botocore<1.13.0,>=1.12.238->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nimport nltk\\nnltk.download('stopwords')\\nnltk.download('punkt')\\nnltk.download('wordnet')\\nimport sklearn\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install gensim\n",
    "\"\"\"\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import sklearn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          1  With all this stuff going down at the moment w...\n",
       "1          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2          0  The film starts with a manager (Nicholas Bell)...\n",
       "3          0  It must be assumed that those who praised this...\n",
       "4          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "traindf = pd.read_csv('labeledTrainData.tsv', delimiter=\"\\t\")\n",
    "traindf = traindf.drop(['id'], axis=1)\n",
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews:  25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>processed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>[stuff, go, moment, mj, ive, start, listen, mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "      <td>[classic, war, worlds, timothy, hines, enterta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>[film, start, manager, nicholas, bell, give, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review  \\\n",
       "0          1  With all this stuff going down at the moment w...   \n",
       "1          1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
       "2          0  The film starts with a manager (Nicholas Bell)...   \n",
       "\n",
       "                                    processed_review  \n",
       "0  [stuff, go, moment, mj, ive, start, listen, mu...  \n",
       "1  [classic, war, worlds, timothy, hines, enterta...  \n",
       "2  [film, start, manager, nicholas, bell, give, w...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "# import the inflect library \n",
    "import inflect \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "p = inflect.engine() \n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "# remove stopwords function \n",
    "def remove_stopwords(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    filtered_text = \" \".join(filtered_text)\n",
    "    return filtered_text \n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "  \n",
    "# convert number into words \n",
    "def convert_number(text): \n",
    "    # split string into list of words \n",
    "    temp_str = text.split() \n",
    "    # initialise empty list \n",
    "    new_string = [] \n",
    "  \n",
    "    for word in temp_str: \n",
    "        # if word is a digit, convert the digit \n",
    "        # to numbers and append into the new_string list \n",
    "        if word.isdigit(): \n",
    "            temp = p.number_to_words(word) \n",
    "            new_string.append(temp) \n",
    "  \n",
    "        # append the word as it is \n",
    "        else: \n",
    "            new_string.append(word) \n",
    "  \n",
    "    # join the words of new_string to form a string \n",
    "    temp_str = ' '.join(new_string) \n",
    "    return temp_str\n",
    "\n",
    "# remove punctuation \n",
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator)\n",
    "\n",
    "# remove whitespace from text \n",
    "def remove_whitespace(text): \n",
    "    return  \" \".join(text.split()) \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "# lemmatize string \n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    # provide context i.e. part-of-speech \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    #lemmas = \" \".join(lemmas)\n",
    "    return lemmas \n",
    "# stem words in the list of tokenised words \n",
    "def stem_words(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stemmer.stem(word) for word in word_tokens] \n",
    "    stems = \" \".join(stems)\n",
    "    return stems \n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text_lowercase(text)\n",
    "    text = convert_number(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = remove_stopwords(text)\n",
    "    #text = stem_words(text)\n",
    "    text = lemmatize_word(text)\n",
    "    return text\n",
    "\n",
    "traindf['processed_review'] = traindf.review.apply(lambda x: preprocess_text(x))  \n",
    "print(\"Number of reviews: \",len(traindf))\n",
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom keras.preprocessing.text import Tokenizer\\nmax_features = 6000\\ntokenizer = Tokenizer(num_words=max_features)\\ntokenizer.fit_on_texts(traindf['processed_review'])\\nlist_tokenized_train = tokenizer.texts_to_sequences(traindf['processed_review'])\\nmaxlen = 200\\nX_pad = pad_sequences(list_tokenized_train, maxlen=maxlen)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BAG OF WORDS MODEL\n",
    "\"\"\"\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(traindf['processed_review'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(traindf['processed_review'])\n",
    "maxlen = 200\n",
    "X_pad = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec MODEL\n",
    "from gensim.models import Word2Vec\n",
    "embedding_vector_size = 300\n",
    "wordvec_model = Word2Vec(\n",
    "    sentences = traindf['processed_review'],\n",
    "    size = embedding_vector_size,\n",
    "    min_count=3, window=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36749\n",
      "Number of reviews:  25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('excellent', 0.7485159039497375),\n",
       " ('wonderful', 0.7191956043243408),\n",
       " ('fantastic', 0.7094447016716003),\n",
       " ('amaze', 0.6841191649436951),\n",
       " ('fine', 0.6785985231399536),\n",
       " ('incredible', 0.6731829643249512),\n",
       " ('awesome', 0.6718109846115112),\n",
       " ('perfect', 0.6689537763595581),\n",
       " ('good', 0.648886501789093),\n",
       " ('nice', 0.5896484851837158)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", len(wordvec_model.wv.vocab))\n",
    "print(\"Number of reviews: \",len(traindf))\n",
    "wordvec_model.wv.most_similar(\"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize sentences... (done)\n"
     ]
    }
   ],
   "source": [
    "# Code grabbed from https://www.kaggle.com/alexcherniuk/imdb-review-word2vec-bilstm-99-acc\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def vectorize_data(data, vocab: dict) -> list:\n",
    "    print('Vectorize sentences...', end='\\r')\n",
    "    keys = list(vocab.keys())\n",
    "    filter_unknown = lambda word: vocab.get(word, None) is not None\n",
    "    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
    "    vectorized = list(map(encode, data))\n",
    "    print('Vectorize sentences... (done)')\n",
    "    return vectorized\n",
    "\n",
    "embed_size = 150\n",
    "X_pad = pad_sequences(\n",
    "    sequences=vectorize_data(traindf['processed_review'], vocab=wordvec_model.wv.vocab),\n",
    "    maxlen=embed_size,\n",
    "    padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = traindf['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0930 06:33:48.302227  7408 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0930 06:33:48.319187  7408 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0930 06:33:48.323176  7408 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0930 06:33:48.337136  7408 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0930 06:33:48.338171  7408 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0930 06:33:50.870783  7408 deprecation.py:506] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 300)          11024700  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               187392    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 11,212,221\n",
      "Trainable params: 11,212,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten,CuDNNLSTM\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "embed_size = 150\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=wordvec_model.wv.vectors.shape[0],output_dim=wordvec_model.wv.vectors.shape[1],weights=[wordvec_model.wv.vectors], input_length=embed_size))\n",
    "model.add(Bidirectional(CuDNNLSTM(64)))\n",
    "#model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.1))\n",
    "#model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  383,   653,   138, ...,     0,     0,     0],\n",
       "       [   31,  1373, 14058, ...,     0,     0,     0],\n",
       "       [ 1256,  2775,   327, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  327,  1261, 20618, ...,  6392,  3574, 22032],\n",
       "       [ 2031,  4339,  1069, ...,  5551, 16162,  3764],\n",
       "       [ 1126,   120,    67, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42,shuffle=True)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/3\n",
      "16000/16000 [==============================] - 24s 1ms/step - loss: 0.1205 - acc: 0.9624 - val_loss: 0.1431 - val_acc: 0.9557\n",
      "Epoch 2/3\n",
      "16000/16000 [==============================] - 23s 1ms/step - loss: 0.0510 - acc: 0.9858 - val_loss: 0.1736 - val_acc: 0.9453\n",
      "Epoch 3/3\n",
      "16000/16000 [==============================] - 23s 1ms/step - loss: 0.0253 - acc: 0.9936 - val_loss: 0.1904 - val_acc: 0.9447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x216e40b4278>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 3\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 2s 335us/step\n",
      "\n",
      "Test accuracy: 95.1%\n"
     ]
    }
   ],
   "source": [
    "# validate the model on test dataset to determine generalization\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf=pd.read_csv(\"testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\n",
    "testdf.head(3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
