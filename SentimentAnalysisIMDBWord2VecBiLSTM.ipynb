{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words meets Bag of Popcorn\n",
    "A Kaggle Competition using Google's Word2Vec for movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set\n",
    "The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the word2vec, we use all available data sets and combine to form a big enough corpus. Also, to increase our accuracy, we use an additional augmented dataset with a total of 49,912 reviews which also contains the original 25,000 train data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we should make sure that no entry in the test data set is in the augmented data set. This is confirmed by the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49912 entries, 0 to 49911\n",
      "Data columns (total 3 columns):\n",
      "sentiment    49912 non-null int64\n",
      "review       49912 non-null object\n",
      "type         49912 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Data columns (total 4 columns):\n",
      "id           0 non-null object\n",
      "review       0 non-null object\n",
      "sentiment    0 non-null int64\n",
      "type         0 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 0.0+ bytes\n",
      "None\n",
      "Empty DataFrame\n",
      "Columns: [id, review, sentiment, type]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "testdf = pd.read_csv('testData.tsv',sep=\"\\t\")\n",
    "traindf = pd.read_csv('augmented_traindata_clean2.csv')\n",
    "print(traindf.info())\n",
    "mergedf = pd.merge(testdf,traindf,on=['review'],how='inner')\n",
    "print(mergedf.info())\n",
    "print(mergedf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine all the datasets into one review corpus which we'll use only for the Word2vec model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews:  124912\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "traindf=[]\n",
    "traindf = pd.read_csv('augmented_traindata_clean2.csv')\n",
    "#traindf = traindf.drop(['id'], axis=1)\n",
    "traindf['type'] = 'train'\n",
    "addtldf=[]\n",
    "addtldf = pd.read_csv('unlabeledTrainData.tsv',error_bad_lines=False,delimiter=\"\\t\")\n",
    "addtldf = addtldf.drop(['id'], axis=1)\n",
    "addtldf['type'] = 'addtl'\n",
    "test=[]\n",
    "test = pd.read_csv('testData.tsv',delimiter=\"\\t\")\n",
    "test['type'] = 'test'\n",
    "revcorpus = pd.concat([traindf,addtldf,test],ignore_index=True,sort=False)\n",
    "print(\"Number of reviews: \",len(revcorpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "We perform data cleaning on the reviews by removing tags, removing stopwords, lemmatization, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews:  124912\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "# import the inflect library \n",
    "import inflect \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams,bigrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import download,set_proxy\n",
    "#download('punkt')\n",
    "#download('stopwords')\n",
    "#download('wordnet')\n",
    "stemmer = PorterStemmer() \n",
    "p = inflect.engine() \n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "# extract pure text from html\n",
    "def remove_tags(text):\n",
    "    try:\n",
    "        text = re.sub(r'<[^<>]+>', \" \", text)\n",
    "    except:\n",
    "        print(text)\n",
    "    return text\n",
    "# remove stopwords function \n",
    "def remove_stopwords(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    filtered_text = \" \".join(filtered_text)\n",
    "    return filtered_text \n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "  \n",
    "# convert number into words \n",
    "def convert_number(text): \n",
    "    # split string into list of words \n",
    "    temp_str = text.split() \n",
    "    # initialise empty list \n",
    "    new_string = [] \n",
    "  \n",
    "    for word in temp_str: \n",
    "        # if word is a digit, convert the digit \n",
    "        # to numbers and append into the new_string list \n",
    "        if word.isdigit(): \n",
    "            temp = p.number_to_words(word) \n",
    "            new_string.append(temp) \n",
    "  \n",
    "        # append the word as it is \n",
    "        else: \n",
    "            new_string.append(word) \n",
    "  \n",
    "    # join the words of new_string to form a string \n",
    "    temp_str = ' '.join(new_string) \n",
    "    return temp_str\n",
    "\n",
    "# remove punctuation \n",
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator)\n",
    "\n",
    "    \n",
    "# remove whitespace from text \n",
    "def remove_whitespace(text): \n",
    "    return  \" \".join(text.split()) \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "# lemmatize string \n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    # provide context i.e. part-of-speech \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    #lemmas = \" \".join(lemmas)\n",
    "    return lemmas \n",
    "# stem words in the list of tokenised words \n",
    "def stem_words(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stemmer.stem(word) for word in word_tokens] \n",
    "    stems = \" \".join(stems)\n",
    "    return stems \n",
    "\n",
    "def generate_ngram(text,n):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    tokens = word_tokenize(text) \n",
    "    output = list(ngrams(tokens, n))\n",
    "    #output = bigrams(tokens)\n",
    "    return output\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = remove_tags(text)\n",
    "    text = convert_number(text)\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_whitespace(text)\n",
    "    #text = stem_words(text)\n",
    "    #text = ngram_vector(text)\n",
    "    #print(text)\n",
    "    text = lemmatize_word(text)\n",
    "    return text\n",
    "#revcorpus = revcorpus[1:5]\n",
    "revcorpus['processed_review'] = revcorpus.review.apply(lambda x: preprocess_text(x))  \n",
    "#revcorpus['bigrams'] = revcorpus.processed_review.apply(lambda x:generate_ngram(x,2))\n",
    "print(\"Number of reviews: \",len(revcorpus))\n",
    "#print(revcorpus.head(3))\n",
    "#traindf['processed_review'] = traindf.review.apply(lambda x: preprocess_text(x))  \n",
    "#print(\"Number of reviews: \",len(traindf))\n",
    "#traindf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import Phrases\n",
    "reviews = revcorpus['processed_review']\n",
    "reviews.head()\n",
    "bigrams = Phrases(sentences=reviews)\n",
    "trigrams = Phrases(sentences=bigrams[reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['must', 'assume', 'praise', 'film', 'greatest', 'film', 'opera', 'ever', 'didnt', 'read_somewhere', 'either', 'dont_care', 'opera', 'dont_care', 'wagner', 'dont_care', 'anything', 'except', 'desire', 'appear', 'culture', 'either', 'representation', 'wagners', 'swansong', 'movie', 'strike', 'unmitigated_disaster', 'leaden', 'read', 'score', 'match', 'tricksy', 'lugubrious', 'realisation', 'text', 'questionable', 'people', 'ideas', 'opera', 'matter', 'play', 'especially', 'one', 'shakespeare', 'allow', 'anywhere_near', 'theatre', 'film', 'studio', 'syberberg', 'fashionably', 'without', 'smallest', 'justification', 'wagners', 'text', 'decide', 'parsifal', 'bisexual', 'integration', 'title', 'character', 'latter_stag', 'transmute', 'kind', 'beatnik', 'babe', 'though', 'one', 'continue', 'sing', 'high', 'tenor', 'actors', 'film', 'singers', 'get', 'double_dose', 'armin_jordan', 'conductor', 'see', 'face', 'hear', 'voice', 'amfortas', 'also', 'appear', 'monstrously', 'double_exposure', 'kind', 'batonzilla', 'conductor', 'eat', 'monsalvat', 'play', 'good', 'friday', 'music', 'way', 'transcendant', 'loveliness', 'nature', 'represent', 'scatter', 'shopworn', 'flaccid', 'crocuses', 'stick', 'illlaid', 'turf', 'expedient', 'baffle', 'theatre', 'sometimes', 'piece', 'imperfections', 'thoughts', 'cant', 'think', 'syberberg', 'couldnt', 'splice', 'parsifal', 'gurnemanz', 'mountain', 'pasture', 'lush', 'provide', 'julie_andrews', 'sound', 'music', 'sound', 'hard', 'endure', 'high', 'voice', 'trumpet', 'particular', 'possess', 'aural', 'glare', 'add', 'another', 'sort', 'fatigue', 'impatience', 'uninspired', 'conduct', 'paralytic', 'unfold', 'ritual', 'someone', 'another', 'review', 'mention', 'one_thousand', 'nine_hundred', 'fiftyone', 'bayreuth', 'record', 'knappertsbusch', 'though', 'tempi', 'often', 'slow', 'jordan', 'altogether', 'lack', 'sense', 'pulse', 'feel', 'ebb_flow', 'music', 'half_century', 'orchestral', 'sound', 'set', 'modern', 'press', 'still', 'superior', 'film']\n",
      "['must', 'assume', 'praise', 'film', 'greatest', 'film', 'opera', 'ever', 'didnt', 'read_somewhere', 'either', 'dont_care', 'opera', 'dont_care', 'wagner', 'dont_care', 'anything', 'except', 'desire', 'appear', 'culture', 'either', 'representation', 'wagners', 'swansong', 'movie', 'strike', 'unmitigated_disaster', 'leaden', 'read', 'score', 'match', 'tricksy', 'lugubrious', 'realisation', 'text', 'questionable', 'people', 'ideas', 'opera', 'matter', 'play', 'especially', 'one', 'shakespeare', 'allow', 'anywhere_near', 'theatre', 'film', 'studio', 'syberberg', 'fashionably', 'without', 'smallest', 'justification', 'wagners', 'text', 'decide', 'parsifal', 'bisexual', 'integration', 'title', 'character', 'latter_stag', 'transmute', 'kind', 'beatnik', 'babe', 'though', 'one', 'continue', 'sing', 'high', 'tenor', 'actors', 'film', 'singers', 'get', 'double_dose', 'armin_jordan', 'conductor', 'see', 'face', 'hear', 'voice', 'amfortas', 'also', 'appear', 'monstrously', 'double_exposure', 'kind', 'batonzilla', 'conductor', 'eat', 'monsalvat', 'play', 'good', 'friday', 'music', 'way', 'transcendant', 'loveliness', 'nature', 'represent', 'scatter', 'shopworn', 'flaccid', 'crocuses', 'stick', 'illlaid', 'turf', 'expedient', 'baffle', 'theatre', 'sometimes', 'piece', 'imperfections', 'thoughts', 'cant', 'think', 'syberberg', 'couldnt', 'splice', 'parsifal', 'gurnemanz', 'mountain', 'pasture', 'lush', 'provide', 'julie_andrews', 'sound', 'music', 'sound', 'hard', 'endure', 'high', 'voice', 'trumpet', 'particular', 'possess', 'aural', 'glare', 'add', 'another', 'sort', 'fatigue', 'impatience', 'uninspired', 'conduct', 'paralytic', 'unfold', 'ritual', 'someone', 'another', 'review', 'mention', 'one_thousand_nine_hundred', 'fiftyone', 'bayreuth', 'record', 'knappertsbusch', 'though', 'tempi', 'often', 'slow', 'jordan', 'altogether', 'lack', 'sense', 'pulse', 'feel', 'ebb_flow', 'music', 'half_century', 'orchestral', 'sound', 'set', 'modern', 'press', 'still', 'superior', 'film']\n"
     ]
    }
   ],
   "source": [
    "print(bigrams[reviews][2])\n",
    "print(trigrams[bigrams[reviews]][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Word2Vec MODEL\n",
    "from gensim.models import Word2Vec\n",
    "embedding_vector_size = 300\n",
    "#tokens = word_tokenize(bigrams[revcorpus['processed_review']])\n",
    "wordvec_model = Word2Vec(\n",
    "    sentences = reviews,\n",
    "    size = embedding_vector_size,\n",
    "    min_count=3, window=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 82653\n",
      "Number of reviews:  49912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('wonderful', 0.6779197454452515),\n",
       " ('fantastic', 0.6756469011306763),\n",
       " ('excellent', 0.6671639084815979),\n",
       " ('terrific', 0.6359677314758301),\n",
       " ('good', 0.6292046308517456),\n",
       " ('awesome', 0.5692603588104248),\n",
       " ('fine', 0.5664957165718079),\n",
       " ('outstanding', 0.5593339204788208),\n",
       " ('superb', 0.5544353127479553),\n",
       " ('fabulous', 0.5517513155937195)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "print(\"Vocabulary size:\", len(wordvec_model.wv.vocab))\n",
    "print(\"Number of reviews: \",len(traindf))\n",
    "#for word in wordvec_model.wv.vocab:\n",
    "#    print((word, wordvec_model.wv.vocab[word].count))\n",
    "wordvec_model.wv.most_similar(\"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49912\n"
     ]
    }
   ],
   "source": [
    "train_reviews = revcorpus[revcorpus['type']==\"train\"][\"processed_review\"]\n",
    "#train_reviews = trigrams[bigrams[train_reviews]]\n",
    "#train_reviews = [\" \".join(text) for text in [review for review in train_reviews]]\n",
    "print(len(train_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['film',\n",
       " 'start',\n",
       " 'manager',\n",
       " 'nicholas',\n",
       " 'bell',\n",
       " 'give',\n",
       " 'welcome',\n",
       " 'investors',\n",
       " 'robert',\n",
       " 'carradine',\n",
       " 'primal',\n",
       " 'park',\n",
       " 'secret',\n",
       " 'project',\n",
       " 'mutate',\n",
       " 'primal',\n",
       " 'animal',\n",
       " 'use',\n",
       " 'fossilize',\n",
       " 'dna',\n",
       " 'like',\n",
       " '¨jurassik',\n",
       " 'park¨',\n",
       " 'scientists',\n",
       " 'resurrect',\n",
       " 'one',\n",
       " 'natures',\n",
       " 'fearsome',\n",
       " 'predators',\n",
       " 'sabretooth',\n",
       " 'tiger',\n",
       " 'smilodon',\n",
       " 'scientific',\n",
       " 'ambition',\n",
       " 'turn',\n",
       " 'deadly',\n",
       " 'however',\n",
       " 'high',\n",
       " 'voltage',\n",
       " 'fence',\n",
       " 'open',\n",
       " 'creature',\n",
       " 'escape',\n",
       " 'begin',\n",
       " 'savagely',\n",
       " 'stalk',\n",
       " 'prey',\n",
       " 'human',\n",
       " 'visitors',\n",
       " 'tourists',\n",
       " 'scientificmeanwhile',\n",
       " 'youngsters',\n",
       " 'enter',\n",
       " 'restrict',\n",
       " 'area',\n",
       " 'security',\n",
       " 'center',\n",
       " 'attack',\n",
       " 'pack',\n",
       " 'large',\n",
       " 'prehistorical',\n",
       " 'animals',\n",
       " 'deadlier',\n",
       " 'bigger',\n",
       " 'addition',\n",
       " 'security',\n",
       " 'agent',\n",
       " 'stacy',\n",
       " 'haiduk',\n",
       " 'mate',\n",
       " 'brian',\n",
       " 'wimmer',\n",
       " 'fight',\n",
       " 'hardly',\n",
       " 'carnivorous',\n",
       " 'smilodons',\n",
       " 'sabretooths',\n",
       " 'course',\n",
       " 'real',\n",
       " 'star',\n",
       " 'star',\n",
       " 'astound',\n",
       " 'terrifyingly',\n",
       " 'though',\n",
       " 'convince',\n",
       " 'giant',\n",
       " 'animals',\n",
       " 'savagely',\n",
       " 'stalk',\n",
       " 'prey',\n",
       " 'group',\n",
       " 'run',\n",
       " 'afoul',\n",
       " 'fight',\n",
       " 'one',\n",
       " 'natures',\n",
       " 'fearsome',\n",
       " 'predators',\n",
       " 'furthermore',\n",
       " 'third',\n",
       " 'sabretooth',\n",
       " 'dangerous',\n",
       " 'slow',\n",
       " 'stalk',\n",
       " 'victims',\n",
       " 'movie',\n",
       " 'deliver',\n",
       " 'goods',\n",
       " 'lot',\n",
       " 'blood',\n",
       " 'gore',\n",
       " 'behead',\n",
       " 'hairraising',\n",
       " 'chillsfull',\n",
       " 'scar',\n",
       " 'sabretooths',\n",
       " 'appear',\n",
       " 'mediocre',\n",
       " 'special',\n",
       " 'effectsthe',\n",
       " 'story',\n",
       " 'provide',\n",
       " 'excite',\n",
       " 'stir',\n",
       " 'entertainment',\n",
       " 'result',\n",
       " 'quite',\n",
       " 'bore',\n",
       " 'giant',\n",
       " 'animals',\n",
       " 'majority',\n",
       " 'make',\n",
       " 'computer',\n",
       " 'generator',\n",
       " 'seem',\n",
       " 'totally',\n",
       " 'lousy',\n",
       " 'middle',\n",
       " 'performances',\n",
       " 'though',\n",
       " 'players',\n",
       " 'react',\n",
       " 'appropriately',\n",
       " 'become',\n",
       " 'foodactors',\n",
       " 'give',\n",
       " 'vigorously',\n",
       " 'physical',\n",
       " 'performances',\n",
       " 'dodge',\n",
       " 'beasts',\n",
       " 'runningbound',\n",
       " 'leap',\n",
       " 'dangle',\n",
       " 'wall',\n",
       " 'pack',\n",
       " 'ridiculous',\n",
       " 'final',\n",
       " 'deadly',\n",
       " 'scene',\n",
       " 'small',\n",
       " 'kid',\n",
       " 'realisticgory',\n",
       " 'violent',\n",
       " 'attack',\n",
       " 'scenes',\n",
       " 'film',\n",
       " 'sabretooths',\n",
       " 'smilodon',\n",
       " 'follow',\n",
       " '¨sabretooth2002¨by',\n",
       " 'jam',\n",
       " 'r',\n",
       " 'hickox',\n",
       " 'vanessa',\n",
       " 'angel',\n",
       " 'david',\n",
       " 'keith',\n",
       " 'john',\n",
       " 'rhys',\n",
       " 'davies',\n",
       " 'much',\n",
       " 'better',\n",
       " '¨10000',\n",
       " 'bc2006¨',\n",
       " 'roland',\n",
       " 'emmerich',\n",
       " 'steven',\n",
       " 'strait',\n",
       " 'cliff',\n",
       " 'curtis',\n",
       " 'camilla',\n",
       " 'belle',\n",
       " 'motion',\n",
       " 'picture',\n",
       " 'fill',\n",
       " 'bloody',\n",
       " 'moments',\n",
       " 'badly',\n",
       " 'direct',\n",
       " 'george',\n",
       " 'miller',\n",
       " 'originality',\n",
       " 'take',\n",
       " 'many',\n",
       " 'elements',\n",
       " 'previous',\n",
       " 'film',\n",
       " 'miller',\n",
       " 'australian',\n",
       " 'director',\n",
       " 'usually',\n",
       " 'work',\n",
       " 'television',\n",
       " 'tidal',\n",
       " 'wave',\n",
       " 'journey',\n",
       " 'center',\n",
       " 'earth',\n",
       " 'many',\n",
       " 'others',\n",
       " 'occasionally',\n",
       " 'cinema',\n",
       " 'man',\n",
       " 'snowy',\n",
       " 'river',\n",
       " 'zeus',\n",
       " 'roxannerobinson',\n",
       " 'crusoe',\n",
       " 'rat',\n",
       " 'average',\n",
       " 'bottom',\n",
       " 'barrel']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train_reviews[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization \n",
    "Converting the words to vectors (Code grabbed from https://www.kaggle.com/alexcherniuk/imdb-review-word2vec-bilstm-99-acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize sentences... (done)\n",
      "Wall time: 14min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def vectorize_data(data, vocab: dict) -> list:\n",
    "    print('Vectorize sentences...', end='\\r')\n",
    "    keys = list(vocab.keys())\n",
    "    filter_unknown = lambda word: vocab.get(word, None) is not None\n",
    "    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
    "    vectorized = list(map(encode, data))\n",
    "    print('Vectorize sentences... (done)')\n",
    "    return vectorized\n",
    "embed_size = 150\n",
    "X_pad = pad_sequences(\n",
    "    sequences=vectorize_data(train_reviews, vocab=wordvec_model.wv.vocab),\n",
    "    maxlen=embed_size,\n",
    "    padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([132, 133, 134, 135, 136, 136, 137, 138, 139, 140, 141, 120, 104,\n",
       "       105, 106, 142, 143, 144, 130,  85,  86,  87,  88, 145, 146,  89,\n",
       "       147, 148, 105, 149,  41, 150, 151, 152, 153, 154, 155, 156, 157,\n",
       "       133, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       141, 120, 170,  59, 171, 172, 173, 174, 175, 176, 177, 139, 178,\n",
       "       179, 180, 181,  68, 182, 183, 177, 184, 185, 186, 187, 188, 117,\n",
       "       189, 190,  95, 191, 192, 193, 194, 116, 195,   6, 133,  91, 196,\n",
       "       197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "       210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222,\n",
       "       223, 224, 225, 226, 227, 228, 229, 230,   6, 225, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 115, 239, 228,  47, 240, 241, 242, 243,\n",
       "       244, 245, 246, 247, 248, 249, 250])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = revcorpus[revcorpus['type']==\"train\"]['sentiment']\n",
    "#y = traindf['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1008 22:19:45.498079 22240 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1008 22:19:45.521021 22240 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1008 22:19:45.526208 22240 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1008 22:19:45.542958 22240 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1008 22:19:45.542958 22240 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 300)          24795900  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 150, 256)          440320    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 25,236,477\n",
      "Trainable params: 25,236,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten,CuDNNLSTM, CuDNNGRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "embed_size = 150\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=wordvec_model.wv.vectors.shape[0],output_dim=wordvec_model.wv.vectors.shape[1],weights=[wordvec_model.wv.vectors], input_length=embed_size,trainable=True))\n",
    "#model.add(Embedding(max_features,embed_size))\n",
    "model.add(Bidirectional(CuDNNLSTM(128,return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "#model.add(Dropout(0.1))\n",
    "#model.add(Dense(20,activation='relu'))\n",
    "#model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  881,  1012,  3827, ...,     0,     0,     0],\n",
       "       [ 4784,   949,  1439, ...,     0,     0,     0],\n",
       "       [ 2990,   530,   136, ...,  1306,   160,    41],\n",
       "       ...,\n",
       "       [   79, 16673, 26434, ...,   194,   782, 13764],\n",
       "       [  748,  1264,   805, ...,     0,     0,     0],\n",
       "       [ 6619,  2264,   418, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, shuffle=False)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1008 22:19:49.101409 22240 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1008 22:19:49.153273 22240 deprecation.py:323] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39929 samples, validate on 9983 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "2 root error(s) found.\n  (0) Unknown: Fail to find the dnn implementation.\n\t [[{{node bidirectional_1/CudnnRNN}}]]\n\t [[loss/mul/_123]]\n  (1) Unknown: Fail to find the dnn implementation.\n\t [[{{node bidirectional_1/CudnnRNN}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4dd38e58012e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown: Fail to find the dnn implementation.\n\t [[{{node bidirectional_1/CudnnRNN}}]]\n\t [[loss/mul/_123]]\n  (1) Unknown: Fail to find the dnn implementation.\n\t [[{{node bidirectional_1/CudnnRNN}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 20\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train, validation_data=(X_test,y_test),batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# validate the model on test dataset to determine generalization\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb=pd.read_csv(\"imdb_master.csv\",encoding=\"latin-1\")\n",
    "imdb = imdb[imdb.type==\"test\"]\n",
    "imdb['label'] = imdb['label'].map({'neg':0,'pos':1})\n",
    "imdb['processed_review'] = imdb.review.apply(lambda x: preprocess_text(x))  \n",
    "\n",
    "print(imdb.info())\n",
    "imdb.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pad_sequences(\n",
    "    sequences=vectorize_data(imdb['processed_review'], vocab=wordvec_model.wv.vocab),\n",
    "    maxlen=embed_size,\n",
    "    padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = imdb['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
