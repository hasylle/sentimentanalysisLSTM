{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 43043: expected 2 fields, saw 3\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews:  74998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 10\n",
    "traindf=[]\n",
    "traindf = pd.read_csv('labeledTrainData.tsv', delimiter=\"\\t\")\n",
    "traindf = traindf.drop(['id'], axis=1)\n",
    "traindf['type'] = 'train'\n",
    "addtldf=[]\n",
    "addtldf = pd.read_csv('unlabeledTrainData.tsv',error_bad_lines=False,delimiter=\"\\t\")\n",
    "addtldf = addtldf.drop(['id'], axis=1)\n",
    "addtldf['type'] = 'addtl'\n",
    "revcorpus = pd.concat([traindf,addtldf],ignore_index=True)\n",
    "print(\"Number of reviews: \",len(revcorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews:  74998\n",
      "      review  sentiment   type processed_review\n",
      "0  With a...        1.0  train  [stuff...      \n",
      "1  \\The C...        1.0  train  [class...      \n",
      "2  The fi...        0.0  train  [film,...      \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "# import the inflect library \n",
    "import inflect \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams,bigrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "p = inflect.engine() \n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "# extract pure text from html\n",
    "def remove_tags(text):\n",
    "    text = re.sub(r'<[^<>]+>', \" \", text)\n",
    "    return text\n",
    "# remove stopwords function \n",
    "def remove_stopwords(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    filtered_text = \" \".join(filtered_text)\n",
    "    return filtered_text \n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "  \n",
    "# convert number into words \n",
    "def convert_number(text): \n",
    "    # split string into list of words \n",
    "    temp_str = text.split() \n",
    "    # initialise empty list \n",
    "    new_string = [] \n",
    "  \n",
    "    for word in temp_str: \n",
    "        # if word is a digit, convert the digit \n",
    "        # to numbers and append into the new_string list \n",
    "        if word.isdigit(): \n",
    "            temp = p.number_to_words(word) \n",
    "            new_string.append(temp) \n",
    "  \n",
    "        # append the word as it is \n",
    "        else: \n",
    "            new_string.append(word) \n",
    "  \n",
    "    # join the words of new_string to form a string \n",
    "    temp_str = ' '.join(new_string) \n",
    "    return temp_str\n",
    "\n",
    "# remove punctuation \n",
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator)\n",
    "\n",
    "    \n",
    "# remove whitespace from text \n",
    "def remove_whitespace(text): \n",
    "    return  \" \".join(text.split()) \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "# lemmatize string \n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    # provide context i.e. part-of-speech \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    #lemmas = \" \".join(lemmas)\n",
    "    return lemmas \n",
    "# stem words in the list of tokenised words \n",
    "def stem_words(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stemmer.stem(word) for word in word_tokens] \n",
    "    stems = \" \".join(stems)\n",
    "    return stems \n",
    "\n",
    "def generate_ngram(text,n):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    tokens = word_tokenize(text) \n",
    "    output = list(ngrams(tokens, n))\n",
    "    #output = bigrams(tokens)\n",
    "    return output\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = remove_tags(text)\n",
    "    text = convert_number(text)\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_whitespace(text)\n",
    "    #text = stem_words(text)\n",
    "    #text = ngram_vector(text)\n",
    "    #print(text)\n",
    "    text = lemmatize_word(text)\n",
    "    return text\n",
    "#revcorpus = revcorpus[1:5]\n",
    "revcorpus['processed_review'] = revcorpus.review.apply(lambda x: preprocess_text(x))  \n",
    "#revcorpus['bigrams'] = revcorpus.processed_review.apply(lambda x:generate_ngram(x,2))\n",
    "print(\"Number of reviews: \",len(revcorpus))\n",
    "print(revcorpus.head(3))\n",
    "#traindf['processed_review'] = traindf.review.apply(lambda x: preprocess_text(x))  \n",
    "#print(\"Number of reviews: \",len(traindf))\n",
    "#traindf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "reviews = revcorpus['processed_review']\n",
    "bigrams = Phrases(sentences=reviews)\n",
    "trigrams = Phrases(sentences=bigrams[reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'start', 'manager', 'nicholas', 'bell', 'give', 'welcome', 'investors', 'robert_carradine', 'primal', 'park', 'secret', 'project', 'mutate', 'primal', 'animal', 'use', 'fossilize', 'dna', 'like', '¨jurassik', 'park¨', 'scientists', 'resurrect', 'one', 'natures', 'fearsome', 'predators', 'sabretooth', 'tiger', 'smilodon', 'scientific', 'ambition', 'turn', 'deadly', 'however', 'high_voltage', 'fence', 'open', 'creature', 'escape', 'begin', 'savagely', 'stalk_prey', 'human', 'visitors', 'tourists', 'scientificmeanwhile', 'youngsters', 'enter', 'restrict_area', 'security', 'center', 'attack', 'pack', 'large', 'prehistorical', 'animals', 'deadlier', 'bigger', 'addition', 'security', 'agent', 'stacy', 'haiduk', 'mate', 'brian', 'wimmer', 'fight', 'hardly', 'carnivorous', 'smilodons', 'sabretooths', 'course', 'real', 'star', 'star', 'astound', 'terrifyingly', 'though', 'convince', 'giant', 'animals', 'savagely', 'stalk_prey', 'group', 'run_afoul', 'fight', 'one', 'natures', 'fearsome', 'predators', 'furthermore', 'third', 'sabretooth', 'dangerous', 'slow', 'stalk', 'victims', 'movie', 'deliver_goods', 'lot', 'blood_gore', 'behead', 'hairraising', 'chillsfull', 'scar', 'sabretooths', 'appear', 'mediocre', 'special', 'effectsthe', 'story', 'provide', 'excite', 'stir', 'entertainment', 'result', 'quite', 'bore', 'giant', 'animals', 'majority', 'make', 'computer', 'generator', 'seem', 'totally', 'lousy', 'middle', 'performances', 'though', 'players', 'react', 'appropriately', 'become', 'foodactors', 'give', 'vigorously', 'physical', 'performances', 'dodge', 'beasts', 'runningbound', 'leap', 'dangle', 'wall', 'pack', 'ridiculous', 'final', 'deadly', 'scene', 'small', 'kid', 'realisticgory', 'violent', 'attack', 'scenes', 'film', 'sabretooths', 'smilodon', 'follow', '¨sabretooth2002¨by', 'jam', 'r', 'hickox', 'vanessa_angel', 'david_keith', 'john_rhys', 'davies', 'much_better', '¨10000', 'bc2006¨', 'roland_emmerich', 'steven', 'strait', 'cliff', 'curtis', 'camilla', 'belle', 'motion_picture', 'fill', 'bloody', 'moments', 'badly_direct', 'george', 'miller', 'originality', 'take', 'many', 'elements', 'previous', 'film', 'miller', 'australian', 'director', 'usually', 'work', 'television', 'tidal_wave', 'journey_center', 'earth', 'many_others', 'occasionally', 'cinema', 'man_snowy', 'river', 'zeus', 'roxannerobinson', 'crusoe', 'rat', 'average', 'bottom_barrel']\n",
      "['film', 'start', 'manager', 'nicholas', 'bell', 'give', 'welcome', 'investors', 'robert_carradine', 'primal', 'park', 'secret', 'project', 'mutate', 'primal', 'animal', 'use', 'fossilize', 'dna', 'like', '¨jurassik', 'park¨', 'scientists', 'resurrect', 'one', 'natures', 'fearsome', 'predators', 'sabretooth', 'tiger', 'smilodon', 'scientific', 'ambition', 'turn', 'deadly', 'however', 'high_voltage', 'fence', 'open', 'creature', 'escape', 'begin', 'savagely', 'stalk_prey', 'human', 'visitors', 'tourists', 'scientificmeanwhile', 'youngsters', 'enter', 'restrict_area', 'security', 'center', 'attack', 'pack', 'large', 'prehistorical', 'animals', 'deadlier', 'bigger', 'addition', 'security', 'agent', 'stacy', 'haiduk', 'mate', 'brian', 'wimmer', 'fight', 'hardly', 'carnivorous', 'smilodons', 'sabretooths', 'course', 'real', 'star', 'star', 'astound', 'terrifyingly', 'though', 'convince', 'giant', 'animals', 'savagely', 'stalk_prey', 'group', 'run_afoul', 'fight', 'one', 'natures', 'fearsome', 'predators', 'furthermore', 'third', 'sabretooth', 'dangerous', 'slow', 'stalk', 'victims', 'movie', 'deliver_goods', 'lot', 'blood_gore', 'behead', 'hairraising', 'chillsfull', 'scar', 'sabretooths', 'appear', 'mediocre', 'special', 'effectsthe', 'story', 'provide', 'excite', 'stir', 'entertainment', 'result', 'quite', 'bore', 'giant', 'animals', 'majority', 'make', 'computer', 'generator', 'seem', 'totally', 'lousy', 'middle', 'performances', 'though', 'players', 'react', 'appropriately', 'become', 'foodactors', 'give', 'vigorously', 'physical', 'performances', 'dodge', 'beasts', 'runningbound', 'leap', 'dangle', 'wall', 'pack', 'ridiculous', 'final', 'deadly', 'scene', 'small', 'kid', 'realisticgory', 'violent', 'attack', 'scenes', 'film', 'sabretooths', 'smilodon', 'follow', '¨sabretooth2002¨by', 'jam', 'r', 'hickox', 'vanessa_angel', 'david_keith', 'john_rhys_davies', 'much_better', '¨10000', 'bc2006¨', 'roland_emmerich', 'steven', 'strait', 'cliff', 'curtis', 'camilla', 'belle', 'motion_picture', 'fill', 'bloody', 'moments', 'badly_direct', 'george', 'miller', 'originality', 'take', 'many', 'elements', 'previous', 'film', 'miller', 'australian', 'director', 'usually', 'work', 'television', 'tidal_wave', 'journey_center_earth', 'many_others', 'occasionally', 'cinema', 'man_snowy_river', 'zeus', 'roxannerobinson', 'crusoe', 'rat', 'average', 'bottom_barrel']\n"
     ]
    }
   ],
   "source": [
    "print(bigrams[reviews][2])\n",
    "print(trigrams[bigrams[reviews]][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec MODEL\n",
    "from gensim.models import Word2Vec\n",
    "embedding_vector_size = 300\n",
    "#tokens = word_tokenize(bigrams[revcorpus['processed_review']])\n",
    "wordvec_model = Word2Vec(\n",
    "    sentences = trigrams[bigrams[reviews]],\n",
    "    size = embedding_vector_size,\n",
    "    min_count=3, window=5, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 94935\n",
      "Number of reviews:  25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('terrific', 0.6434566974639893),\n",
       " ('tremendous', 0.6075615882873535),\n",
       " ('one_favourites', 0.6047403812408447),\n",
       " ('totally_awesome', 0.6020132303237915),\n",
       " ('wonderfully_do', 0.5982685089111328),\n",
       " ('greatthe', 0.5954743027687073),\n",
       " ('absolutely_incredible', 0.5937920808792114),\n",
       " ('fantastic', 0.5927061438560486),\n",
       " ('phenomenal', 0.5917737483978271),\n",
       " ('wonderful', 0.588508129119873)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "print(\"Vocabulary size:\", len(wordvec_model.wv.vocab))\n",
    "print(\"Number of reviews: \",len(traindf))\n",
    "#for word in wordvec_model.wv.vocab:\n",
    "#    print((word, wordvec_model.wv.vocab[word].count))\n",
    "wordvec_model.wv.most_similar(\"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "train_reviews = revcorpus[revcorpus['type']==\"train\"][\"processed_review\"]\n",
    "train_reviews = trigrams[bigrams[train_reviews]]\n",
    "train_reviews = [\" \".join(text) for text in [review for review in train_reviews]]\n",
    "print(len(train_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classic war_worlds timothy_hines entertain film obviously go great effort lengths faithfully recreate h_g_well classic book mr_hines succeed watch film appreciate fact standard predictable hollywood_fare come every_year eg spielberg version tom_cruise slightest resemblance book obviously everyone look different things movie envision amateur critics look criticize everything others rate movie important baseslike entertain people never agree critics enjoy effort mr_hines put faithful hg_well classic_novel find entertain make easy_overlook critics perceive shortcomings'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train_reviews[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize sentences... (done)\n"
     ]
    }
   ],
   "source": [
    "# Code grabbed from https://www.kaggle.com/alexcherniuk/imdb-review-word2vec-bilstm-99-acc\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def vectorize_data(data, vocab: dict) -> list:\n",
    "    print('Vectorize sentences...', end='\\r')\n",
    "    keys = list(vocab.keys())\n",
    "    filter_unknown = lambda word: vocab.get(word, None) is not None\n",
    "    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
    "    vectorized = list(map(encode, data))\n",
    "    print('Vectorize sentences... (done)')\n",
    "    return vectorized\n",
    "embed_size = 150\n",
    "X_pad = pad_sequences(\n",
    "    sequences=vectorize_data(train_reviews, vocab=wordvec_model.wv.vocab),\n",
    "    maxlen=embed_size,\n",
    "    padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4477, 13240, 15855,   966, 15497,   966,  4813,  6755,   793,\n",
       "       15855,  4813, 15855,   302,  4477, 15855,   793, 15497,  9502,\n",
       "        9502, 15855,   302, 15855,  4477, 10718,  4477,  4687,  4813,\n",
       "       15855, 15855,  4477,  4813,  4477, 15855,  6755,   302, 13240,\n",
       "         302, 13240,   793, 15497, 13240,   302, 13240, 11719, 15855,\n",
       "       15855,  4813, 15855,   302, 10718,  4477,  4687, 10718, 15855,\n",
       "         302,   302, 15855,  4813, 15855,  9396,   302,  4477,   966,\n",
       "       15855,   793, 15497, 15855, 15855,  4477, 15855,   302,  4477,\n",
       "        9396, 15855,  9396,   793, 15855,  4477, 15855,  4813, 15855,\n",
       "         302,  4687,   302, 15855, 15855, 13240,   302, 13240, 15855,\n",
       "        4477,  7056, 15855,  9502,  9502,   302,   302, 10718,  4477,\n",
       "       15855,  9396,  6755,  9502, 10718,  9502,  6755,   793, 10718,\n",
       "        4687,  9660, 15855,   793,   793, 13240,   793, 13240,  4477,\n",
       "        4813, 15855,   793,  9502,  4477, 15855,  4477, 15855,   302,\n",
       "        4477, 15497, 15855, 15855,  4813, 15855,   302,   793, 15497,\n",
       "       13240,   302, 13240,  9396, 15855,   302, 13240, 15855,  4813,\n",
       "       15855, 10718,   302, 13240,  4477,  4687])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = revcorpus[revcorpus['type']==\"train\"]['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1005 14:45:56.307834 15752 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1005 14:45:56.326642 15752 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1005 14:45:56.330636 15752 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1005 14:45:56.346592 15752 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1005 14:45:56.347613 15752 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1005 14:46:01.549243 15752 deprecation.py:506] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 300)          28480500  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 150, 64)           64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 28,545,949\n",
      "Trainable params: 28,545,949\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten,CuDNNLSTM, CuDNNGRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "embed_size = 150\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=wordvec_model.wv.vectors.shape[0],output_dim=wordvec_model.wv.vectors.shape[1],weights=[wordvec_model.wv.vectors], input_length=embed_size))\n",
    "#model.add(Embedding(max_features,embed_size))\n",
    "model.add(Bidirectional(CuDNNGRU(32,return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  302, 15855,  6755, ..., 13240,  6755,  4477],\n",
       "       [15855,   302,   793, ...,  4477,   302, 15855],\n",
       "       [15855,  4813, 15855, ...,  9660,   793, 15497],\n",
       "       ...,\n",
       "       [ 4477, 15855,  4477, ...,   793,  4813, 15855],\n",
       "       [  793, 15497, 15855, ..., 13240, 13240,  4477],\n",
       "       [ 4477,  4813,  6755, ...,   302,  6755,  4477]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, shuffle=True)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1005 14:46:01.668599 15752 deprecation_wrapper.py:119] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1005 14:46:01.698519 15752 deprecation.py:323] From C:\\Users\\Ellysah\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "2 root error(s) found.\n  (0) Unknown: Fail to find the dnn implementation.\n\t [[{{node bidirectional_1/CudnnRNN_1}}]]\n\t [[loss/mul/_95]]\n  (1) Unknown: Fail to find the dnn implementation.\n\t [[{{node bidirectional_1/CudnnRNN_1}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-314e73c6194d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'AdaGrad'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown: Fail to find the dnn implementation.\n\t [[{{node bidirectional_1/CudnnRNN_1}}]]\n\t [[loss/mul/_95]]\n  (1) Unknown: Fail to find the dnn implementation.\n\t [[{{node bidirectional_1/CudnnRNN_1}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 3\n",
    "model.compile(loss='binary_crossentropy', optimizer='AdaGrad', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# validate the model on test dataset to determine generalization\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf=pd.read_csv(\"testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\n",
    "testdf.head(3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
