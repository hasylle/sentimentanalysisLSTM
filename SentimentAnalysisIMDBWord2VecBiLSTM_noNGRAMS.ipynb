{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews:  125035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "traindf=[]\n",
    "traindf = pd.read_csv('augmented_traindata_clean.csv')\n",
    "#traindf = traindf.drop(['id'], axis=1)\n",
    "traindf['type'] = 'train'\n",
    "addtldf=[]\n",
    "addtldf = pd.read_csv('unlabeledTrainData.tsv',error_bad_lines=False,delimiter=\"\\t\")\n",
    "addtldf = addtldf.drop(['id'], axis=1)\n",
    "addtldf['type'] = 'addtl'\n",
    "test=[]\n",
    "test = pd.read_csv('testData.tsv',delimiter=\"\\t\")\n",
    "test['type'] = 'test'\n",
    "revcorpus = pd.concat([traindf,addtldf,test],ignore_index=True)\n",
    "print(\"Number of reviews: \",len(revcorpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GULIMANMT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews:  125035\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "# import the inflect library \n",
    "import inflect \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams,bigrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import download,set_proxy\n",
    "set_proxy('https://GULIMANMT:1gigabyte@T@proxysg02.bsp.gov.ph:8888')\n",
    "#download('punkt')\n",
    "#download('stopwords')\n",
    "download('wordnet')\n",
    "stemmer = PorterStemmer() \n",
    "p = inflect.engine() \n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "# extract pure text from html\n",
    "def remove_tags(text):\n",
    "    try:\n",
    "        text = re.sub(r'<[^<>]+>', \" \", text)\n",
    "    except:\n",
    "        print(text)\n",
    "    return text\n",
    "# remove stopwords function \n",
    "def remove_stopwords(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    filtered_text = \" \".join(filtered_text)\n",
    "    return filtered_text \n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "  \n",
    "# convert number into words \n",
    "def convert_number(text): \n",
    "    # split string into list of words \n",
    "    temp_str = text.split() \n",
    "    # initialise empty list \n",
    "    new_string = [] \n",
    "  \n",
    "    for word in temp_str: \n",
    "        # if word is a digit, convert the digit \n",
    "        # to numbers and append into the new_string list \n",
    "        if word.isdigit(): \n",
    "            temp = p.number_to_words(word) \n",
    "            new_string.append(temp) \n",
    "  \n",
    "        # append the word as it is \n",
    "        else: \n",
    "            new_string.append(word) \n",
    "  \n",
    "    # join the words of new_string to form a string \n",
    "    temp_str = ' '.join(new_string) \n",
    "    return temp_str\n",
    "\n",
    "# remove punctuation \n",
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator)\n",
    "\n",
    "    \n",
    "# remove whitespace from text \n",
    "def remove_whitespace(text): \n",
    "    return  \" \".join(text.split()) \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "# lemmatize string \n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    # provide context i.e. part-of-speech \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    #lemmas = \" \".join(lemmas)\n",
    "    return lemmas \n",
    "# stem words in the list of tokenised words \n",
    "def stem_words(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stemmer.stem(word) for word in word_tokens] \n",
    "    stems = \" \".join(stems)\n",
    "    return stems \n",
    "\n",
    "def generate_ngram(text,n):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    tokens = word_tokenize(text) \n",
    "    output = list(ngrams(tokens, n))\n",
    "    #output = bigrams(tokens)\n",
    "    return output\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = remove_tags(text)\n",
    "    text = convert_number(text)\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_whitespace(text)\n",
    "    #text = stem_words(text)\n",
    "    #text = ngram_vector(text)\n",
    "    #print(text)\n",
    "    text = lemmatize_word(text)\n",
    "    return text\n",
    "#revcorpus = revcorpus[1:5]\n",
    "revcorpus['processed_review'] = revcorpus.review.apply(lambda x: preprocess_text(x))  \n",
    "#revcorpus['bigrams'] = revcorpus.processed_review.apply(lambda x:generate_ngram(x,2))\n",
    "print(\"Number of reviews: \",len(revcorpus))\n",
    "#print(revcorpus.head(3))\n",
    "#traindf['processed_review'] = traindf.review.apply(lambda x: preprocess_text(x))  \n",
    "#print(\"Number of reviews: \",len(traindf))\n",
    "#traindf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "reviews = revcorpus['processed_review']\n",
    "reviews.head()\n",
    "bigrams = Phrases(sentences=reviews)\n",
    "trigrams = Phrases(sentences=bigrams[reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['must', 'assume', 'praise', 'film', 'greatest', 'film', 'opera', 'ever', 'didnt', 'read_somewhere', 'either', 'dont_care', 'opera', 'dont_care', 'wagner', 'dont_care', 'anything', 'except', 'desire', 'appear', 'culture', 'either', 'representation', 'wagners', 'swansong', 'movie', 'strike', 'unmitigated_disaster', 'leaden', 'read', 'score', 'match', 'tricksy', 'lugubrious', 'realisation', 'text', 'questionable', 'people', 'ideas', 'opera', 'matter', 'play', 'especially', 'one', 'shakespeare', 'allow', 'anywhere_near', 'theatre', 'film', 'studio', 'syberberg', 'fashionably', 'without', 'smallest', 'justification', 'wagners', 'text', 'decide', 'parsifal', 'bisexual', 'integration', 'title', 'character', 'latter_stag', 'transmute', 'kind', 'beatnik', 'babe', 'though', 'one', 'continue', 'sing', 'high', 'tenor', 'actors', 'film', 'singers', 'get', 'double_dose', 'armin_jordan', 'conductor', 'see', 'face', 'hear', 'voice', 'amfortas', 'also', 'appear', 'monstrously', 'double_exposure', 'kind', 'batonzilla', 'conductor', 'eat', 'monsalvat', 'play', 'good', 'friday', 'music', 'way', 'transcendant', 'loveliness', 'nature', 'represent', 'scatter', 'shopworn', 'flaccid', 'crocuses', 'stick', 'illlaid', 'turf', 'expedient', 'baffle', 'theatre', 'sometimes', 'piece', 'imperfections', 'thoughts', 'cant', 'think', 'syberberg', 'couldnt', 'splice', 'parsifal', 'gurnemanz', 'mountain', 'pasture', 'lush', 'provide', 'julie_andrews', 'sound', 'music', 'sound', 'hard', 'endure', 'high', 'voice', 'trumpet', 'particular', 'possess', 'aural', 'glare', 'add', 'another', 'sort', 'fatigue', 'impatience', 'uninspired', 'conduct', 'paralytic', 'unfold', 'ritual', 'someone', 'another', 'review', 'mention', 'one_thousand', 'nine_hundred', 'fiftyone', 'bayreuth', 'record', 'knappertsbusch', 'though', 'tempi', 'often', 'slow', 'jordan', 'altogether', 'lack', 'sense', 'pulse', 'feel', 'ebb_flow', 'music', 'half_century', 'orchestral', 'sound', 'set', 'modern', 'press', 'still', 'superior', 'film']\n",
      "['must', 'assume', 'praise', 'film', 'greatest', 'film', 'opera', 'ever', 'didnt', 'read_somewhere', 'either', 'dont_care', 'opera', 'dont_care', 'wagner', 'dont_care', 'anything', 'except', 'desire', 'appear', 'culture', 'either', 'representation', 'wagners', 'swansong', 'movie', 'strike', 'unmitigated_disaster', 'leaden', 'read', 'score', 'match', 'tricksy', 'lugubrious', 'realisation', 'text', 'questionable', 'people', 'ideas', 'opera', 'matter', 'play', 'especially', 'one', 'shakespeare', 'allow', 'anywhere_near', 'theatre', 'film', 'studio', 'syberberg', 'fashionably', 'without', 'smallest', 'justification', 'wagners', 'text', 'decide', 'parsifal', 'bisexual', 'integration', 'title', 'character', 'latter_stag', 'transmute', 'kind', 'beatnik', 'babe', 'though', 'one', 'continue', 'sing', 'high', 'tenor', 'actors', 'film', 'singers', 'get', 'double_dose', 'armin_jordan', 'conductor', 'see', 'face', 'hear', 'voice', 'amfortas', 'also', 'appear', 'monstrously', 'double_exposure', 'kind', 'batonzilla', 'conductor', 'eat', 'monsalvat', 'play', 'good', 'friday', 'music', 'way', 'transcendant', 'loveliness', 'nature', 'represent', 'scatter', 'shopworn', 'flaccid', 'crocuses', 'stick', 'illlaid', 'turf', 'expedient', 'baffle', 'theatre', 'sometimes', 'piece', 'imperfections', 'thoughts', 'cant', 'think', 'syberberg', 'couldnt', 'splice', 'parsifal', 'gurnemanz', 'mountain', 'pasture', 'lush', 'provide', 'julie_andrews', 'sound', 'music', 'sound', 'hard', 'endure', 'high', 'voice', 'trumpet', 'particular', 'possess', 'aural', 'glare', 'add', 'another', 'sort', 'fatigue', 'impatience', 'uninspired', 'conduct', 'paralytic', 'unfold', 'ritual', 'someone', 'another', 'review', 'mention', 'one_thousand_nine_hundred', 'fiftyone', 'bayreuth', 'record', 'knappertsbusch', 'though', 'tempi', 'often', 'slow', 'jordan', 'altogether', 'lack', 'sense', 'pulse', 'feel', 'ebb_flow', 'music', 'half_century', 'orchestral', 'sound', 'set', 'modern', 'press', 'still', 'superior', 'film']\n"
     ]
    }
   ],
   "source": [
    "print(bigrams[reviews][2])\n",
    "print(trigrams[bigrams[reviews]][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec MODEL\n",
    "from gensim.models import Word2Vec\n",
    "embedding_vector_size = 300\n",
    "#tokens = word_tokenize(bigrams[revcorpus['processed_review']])\n",
    "wordvec_model = Word2Vec(\n",
    "    sentences = reviews,\n",
    "    size = embedding_vector_size,\n",
    "    min_count=3, window=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 82807\n",
      "Number of reviews:  50035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fantastic', 0.688412070274353),\n",
       " ('wonderful', 0.6839815378189087),\n",
       " ('excellent', 0.6689158082008362),\n",
       " ('terrific', 0.6467872262001038),\n",
       " ('good', 0.62047278881073),\n",
       " ('awesome', 0.5823194980621338),\n",
       " ('superb', 0.5764622688293457),\n",
       " ('outstanding', 0.5740548968315125),\n",
       " ('fine', 0.5605958700180054),\n",
       " ('brilliant', 0.558906078338623)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "print(\"Vocabulary size:\", len(wordvec_model.wv.vocab))\n",
    "print(\"Number of reviews: \",len(traindf))\n",
    "#for word in wordvec_model.wv.vocab:\n",
    "#    print((word, wordvec_model.wv.vocab[word].count))\n",
    "wordvec_model.wv.most_similar(\"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50035\n"
     ]
    }
   ],
   "source": [
    "train_reviews = revcorpus[revcorpus['type']==\"train\"][\"processed_review\"]\n",
    "#train_reviews = trigrams[bigrams[train_reviews]]\n",
    "#train_reviews = [\" \".join(text) for text in [review for review in train_reviews]]\n",
    "print(len(train_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['film',\n",
       " 'start',\n",
       " 'manager',\n",
       " 'nicholas',\n",
       " 'bell',\n",
       " 'give',\n",
       " 'welcome',\n",
       " 'investors',\n",
       " 'robert',\n",
       " 'carradine',\n",
       " 'primal',\n",
       " 'park',\n",
       " 'secret',\n",
       " 'project',\n",
       " 'mutate',\n",
       " 'primal',\n",
       " 'animal',\n",
       " 'use',\n",
       " 'fossilize',\n",
       " 'dna',\n",
       " 'like',\n",
       " '¨jurassik',\n",
       " 'park¨',\n",
       " 'scientists',\n",
       " 'resurrect',\n",
       " 'one',\n",
       " 'natures',\n",
       " 'fearsome',\n",
       " 'predators',\n",
       " 'sabretooth',\n",
       " 'tiger',\n",
       " 'smilodon',\n",
       " 'scientific',\n",
       " 'ambition',\n",
       " 'turn',\n",
       " 'deadly',\n",
       " 'however',\n",
       " 'high',\n",
       " 'voltage',\n",
       " 'fence',\n",
       " 'open',\n",
       " 'creature',\n",
       " 'escape',\n",
       " 'begin',\n",
       " 'savagely',\n",
       " 'stalk',\n",
       " 'prey',\n",
       " 'human',\n",
       " 'visitors',\n",
       " 'tourists',\n",
       " 'scientificmeanwhile',\n",
       " 'youngsters',\n",
       " 'enter',\n",
       " 'restrict',\n",
       " 'area',\n",
       " 'security',\n",
       " 'center',\n",
       " 'attack',\n",
       " 'pack',\n",
       " 'large',\n",
       " 'prehistorical',\n",
       " 'animals',\n",
       " 'deadlier',\n",
       " 'bigger',\n",
       " 'addition',\n",
       " 'security',\n",
       " 'agent',\n",
       " 'stacy',\n",
       " 'haiduk',\n",
       " 'mate',\n",
       " 'brian',\n",
       " 'wimmer',\n",
       " 'fight',\n",
       " 'hardly',\n",
       " 'carnivorous',\n",
       " 'smilodons',\n",
       " 'sabretooths',\n",
       " 'course',\n",
       " 'real',\n",
       " 'star',\n",
       " 'star',\n",
       " 'astound',\n",
       " 'terrifyingly',\n",
       " 'though',\n",
       " 'convince',\n",
       " 'giant',\n",
       " 'animals',\n",
       " 'savagely',\n",
       " 'stalk',\n",
       " 'prey',\n",
       " 'group',\n",
       " 'run',\n",
       " 'afoul',\n",
       " 'fight',\n",
       " 'one',\n",
       " 'natures',\n",
       " 'fearsome',\n",
       " 'predators',\n",
       " 'furthermore',\n",
       " 'third',\n",
       " 'sabretooth',\n",
       " 'dangerous',\n",
       " 'slow',\n",
       " 'stalk',\n",
       " 'victims',\n",
       " 'movie',\n",
       " 'deliver',\n",
       " 'goods',\n",
       " 'lot',\n",
       " 'blood',\n",
       " 'gore',\n",
       " 'behead',\n",
       " 'hairraising',\n",
       " 'chillsfull',\n",
       " 'scar',\n",
       " 'sabretooths',\n",
       " 'appear',\n",
       " 'mediocre',\n",
       " 'special',\n",
       " 'effectsthe',\n",
       " 'story',\n",
       " 'provide',\n",
       " 'excite',\n",
       " 'stir',\n",
       " 'entertainment',\n",
       " 'result',\n",
       " 'quite',\n",
       " 'bore',\n",
       " 'giant',\n",
       " 'animals',\n",
       " 'majority',\n",
       " 'make',\n",
       " 'computer',\n",
       " 'generator',\n",
       " 'seem',\n",
       " 'totally',\n",
       " 'lousy',\n",
       " 'middle',\n",
       " 'performances',\n",
       " 'though',\n",
       " 'players',\n",
       " 'react',\n",
       " 'appropriately',\n",
       " 'become',\n",
       " 'foodactors',\n",
       " 'give',\n",
       " 'vigorously',\n",
       " 'physical',\n",
       " 'performances',\n",
       " 'dodge',\n",
       " 'beasts',\n",
       " 'runningbound',\n",
       " 'leap',\n",
       " 'dangle',\n",
       " 'wall',\n",
       " 'pack',\n",
       " 'ridiculous',\n",
       " 'final',\n",
       " 'deadly',\n",
       " 'scene',\n",
       " 'small',\n",
       " 'kid',\n",
       " 'realisticgory',\n",
       " 'violent',\n",
       " 'attack',\n",
       " 'scenes',\n",
       " 'film',\n",
       " 'sabretooths',\n",
       " 'smilodon',\n",
       " 'follow',\n",
       " '¨sabretooth2002¨by',\n",
       " 'jam',\n",
       " 'r',\n",
       " 'hickox',\n",
       " 'vanessa',\n",
       " 'angel',\n",
       " 'david',\n",
       " 'keith',\n",
       " 'john',\n",
       " 'rhys',\n",
       " 'davies',\n",
       " 'much',\n",
       " 'better',\n",
       " '¨10000',\n",
       " 'bc2006¨',\n",
       " 'roland',\n",
       " 'emmerich',\n",
       " 'steven',\n",
       " 'strait',\n",
       " 'cliff',\n",
       " 'curtis',\n",
       " 'camilla',\n",
       " 'belle',\n",
       " 'motion',\n",
       " 'picture',\n",
       " 'fill',\n",
       " 'bloody',\n",
       " 'moments',\n",
       " 'badly',\n",
       " 'direct',\n",
       " 'george',\n",
       " 'miller',\n",
       " 'originality',\n",
       " 'take',\n",
       " 'many',\n",
       " 'elements',\n",
       " 'previous',\n",
       " 'film',\n",
       " 'miller',\n",
       " 'australian',\n",
       " 'director',\n",
       " 'usually',\n",
       " 'work',\n",
       " 'television',\n",
       " 'tidal',\n",
       " 'wave',\n",
       " 'journey',\n",
       " 'center',\n",
       " 'earth',\n",
       " 'many',\n",
       " 'others',\n",
       " 'occasionally',\n",
       " 'cinema',\n",
       " 'man',\n",
       " 'snowy',\n",
       " 'river',\n",
       " 'zeus',\n",
       " 'roxannerobinson',\n",
       " 'crusoe',\n",
       " 'rat',\n",
       " 'average',\n",
       " 'bottom',\n",
       " 'barrel']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train_reviews[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize sentences... (done)\n"
     ]
    }
   ],
   "source": [
    "# Code grabbed from https://www.kaggle.com/alexcherniuk/imdb-review-word2vec-bilstm-99-acc\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def vectorize_data(data, vocab: dict) -> list:\n",
    "    print('Vectorize sentences...', end='\\r')\n",
    "    keys = list(vocab.keys())\n",
    "    filter_unknown = lambda word: vocab.get(word, None) is not None\n",
    "    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
    "    vectorized = list(map(encode, data))\n",
    "    print('Vectorize sentences... (done)')\n",
    "    return vectorized\n",
    "embed_size = 150\n",
    "X_pad = pad_sequences(\n",
    "    sequences=vectorize_data(train_reviews, vocab=wordvec_model.wv.vocab),\n",
    "    maxlen=embed_size,\n",
    "    padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([132, 133, 134, 135, 136, 136, 137, 138, 139, 140, 141, 120, 104,\n",
       "       105, 106, 142, 143, 144, 130,  85,  86,  87,  88, 145, 146,  89,\n",
       "       147, 148, 105, 149,  41, 150, 151, 152, 153, 154, 155, 156, 157,\n",
       "       133, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       141, 120, 170,  59, 171, 172, 173, 174, 175, 176, 177, 139, 178,\n",
       "       179, 180, 181,  68, 182, 183, 177, 184, 185, 186, 187, 188, 117,\n",
       "       189, 190,  95, 191, 192, 193, 194, 116, 195,   6, 133,  91, 196,\n",
       "       197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "       210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222,\n",
       "       223, 224, 225, 226, 227, 228, 229, 230,   6, 225, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 115, 239, 228,  47, 240, 241, 242, 243,\n",
       "       244, 245, 246, 247, 248, 249, 250])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = revcorpus[revcorpus['type']==\"train\"]['sentiment']\n",
    "#y = traindf['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 150, 300)          24842100  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 150, 256)          439296    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 25,281,653\n",
      "Trainable params: 25,281,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten,CuDNNLSTM, CuDNNGRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "embed_size = 150\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=wordvec_model.wv.vectors.shape[0],output_dim=wordvec_model.wv.vectors.shape[1],weights=[wordvec_model.wv.vectors], input_length=embed_size,trainable=True))\n",
    "#model.add(Embedding(max_features,embed_size))\n",
    "model.add(Bidirectional(LSTM(128,return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "#model.add(Dropout(0.1))\n",
    "#model.add(Dense(20,activation='relu'))\n",
    "#model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1397,  6385,    20, ...,     0,     0,     0],\n",
       "       [  379,   380,   381, ...,  3761,  1546,   208],\n",
       "       [  766, 11249,   152, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 1516,  1711,  3855, ...,     0,     0,     0],\n",
       "       [ 1943,  2195,  1571, ...,  2079,  2154,  2451],\n",
       "       [ 3914, 11633,  3391, ...,  4430, 22656,   803]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, shuffle=True)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40028 samples, validate on 10007 samples\n",
      "Epoch 1/3\n",
      "40028/40028 [==============================] - 296s 7ms/step - loss: 0.3612 - acc: 0.8387 - val_loss: 0.2371 - val_acc: 0.9019\n",
      "Epoch 2/3\n",
      "40028/40028 [==============================] - 332s 8ms/step - loss: 0.1457 - acc: 0.9474 - val_loss: 0.1944 - val_acc: 0.9263\n",
      "Epoch 3/3\n",
      "40028/40028 [==============================] - 413s 10ms/step - loss: 0.0533 - acc: 0.9840 - val_loss: 0.2001 - val_acc: 0.9295\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 3\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train,y_train, validation_data=(X_test,y_test),batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10007/10007 [==============================] - 11s 1ms/step\n",
      "\n",
      "Test accuracy: 93.0%\n"
     ]
    }
   ],
   "source": [
    "# validate the model on test dataset to determine generalization\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25000 entries, 0 to 24999\n",
      "Data columns (total 6 columns):\n",
      "Unnamed: 0          25000 non-null int64\n",
      "type                25000 non-null object\n",
      "review              25000 non-null object\n",
      "label               25000 non-null int64\n",
      "file                25000 non-null object\n",
      "processed_review    25000 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "      <th>processed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the ter...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_2.txt</td>\n",
       "      <td>[mr, costner, drag, movie, far, longer, necessary, aside, terrific, sea, rescue, sequence, care,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of action films are the same. Generic and boring, there's...</td>\n",
       "      <td>0</td>\n",
       "      <td>10000_4.txt</td>\n",
       "      <td>[example, majority, action, film, generic, bore, theres, really, nothing, worth, watch, complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who could'nt act if they had a gun pressed against th...</td>\n",
       "      <td>0</td>\n",
       "      <td>10001_1.txt</td>\n",
       "      <td>[first, hate, moronic, rappers, couldnt, act, gun, press, foreheads, curse, shoot, act, like, cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type  \\\n",
       "0           0  test   \n",
       "1           1  test   \n",
       "2           2  test   \n",
       "\n",
       "                                                                                                review  \\\n",
       "0  Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the ter...   \n",
       "1  This is an example of why the majority of action films are the same. Generic and boring, there's...   \n",
       "2  First of all I hate those moronic rappers, who could'nt act if they had a gun pressed against th...   \n",
       "\n",
       "   label         file  \\\n",
       "0      0      0_2.txt   \n",
       "1      0  10000_4.txt   \n",
       "2      0  10001_1.txt   \n",
       "\n",
       "                                                                                      processed_review  \n",
       "0  [mr, costner, drag, movie, far, longer, necessary, aside, terrific, sea, rescue, sequence, care,...  \n",
       "1  [example, majority, action, film, generic, bore, theres, really, nothing, worth, watch, complete...  \n",
       "2  [first, hate, moronic, rappers, couldnt, act, gun, press, foreheads, curse, shoot, act, like, cl...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb=pd.read_csv(\"imdb_master.csv\",encoding=\"latin-1\")\n",
    "imdb = imdb[imdb.type==\"test\"]\n",
    "imdb['label'] = imdb['label'].map({'neg':0,'pos':1})\n",
    "imdb['processed_review'] = imdb.review.apply(lambda x: preprocess_text(x))  \n",
    "print(imdb.info())\n",
    "imdb.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize sentences... (done)\n"
     ]
    }
   ],
   "source": [
    "X_test = pad_sequences(\n",
    "    sequences=vectorize_data(imdb['processed_review'], vocab=wordvec_model.wv.vocab),\n",
    "    maxlen=embed_size,\n",
    "    padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = imdb['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 29s 1ms/step\n",
      "\n",
      "Test accuracy: 93.0%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
